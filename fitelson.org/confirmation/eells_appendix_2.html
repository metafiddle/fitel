<html><head><meta NAME="GENERATOR" CONTENT="Microsoft FrontPage 3.0"><meta NAME="DATE" CONTENT="10/14/1999"><meta NAME="Author" CONTENT="Dan Hausman"><title>Appendix 2 of Eells</title><meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><style type="text/css"><!--body,td,th {	font-family: Times New Roman, Times, serif;	color: #000000;}body {	background-color: #FFFFFF;}--></style></head><body LINK="#0000ff" VLINK="#551a8b" ALINK="#ff0000"><p>APPENDIX 2: PROBABILITY</p><p>from Ellery Eells, <em>Probabilistic Causality</em>. Cambridge University Press, 1991,pp. 399-402.<br WP="BR2"></p><p>In this appendix, I will present some of the basic ideas of the mathematical theory ofprobability. As in the case of Appendix 1, this will not be a comprehensive or detailedsurvey -- it is only intended to introduce the basic formal probability concepts and rulesused in this book, and to clarify the terminology and notation used in this book. Here Iwill discuss only the <u>abstract and formal</u> calculus of probability; in Chapter 1,the question of <u>interpretation</u> is addressed.</p><p>A probability function, <u>Pr</u>, is any function (or rule of association) thatassigns to (or associates with) each element <u>X</u> of some Boolean algebra <u>B</u>(see Appendix 1) a real number, <u>Pr</u>(<u>X</u>), in accordance with the followingthree conditions:</p><p>For all <u>X</u> and <u>Y</u> in <u>B</u>,</p><p>(1) <u>Pr</u>(<u>X</u>) <u>&gt;</u> 0;</p><p>(2) <u>Pr</u>(<u>X</u>) = 1, if <u>X</u> is a tautology (that is, if <u>X</u> islogically true, or <u>X</u> = <u>1</u> in <u>B</u>);</p><p>(3) <u>Pr</u>(<u>X</u>v<u>Y</u>) = <u>Pr</u>(<u>X</u>) + <u>Pr</u>(<u>Y</u>), if <u>X</u>&amp;<u>Y</u>is a contradiction</p><p>(that is, if <u>X</u>&amp;<u>Y</u> is logically false, or <u>X</u>&amp;<u>Y</u> = <u>0</u>in <u>B</u>).</p><p>These three conditions are the <u>probability axioms</u>, also called &quot;theKolmogorov axioms&quot; (for Kolmogorov 1933). A function <u>Pr</u> that satisfies theaxioms, relative to an algebra <u>B</u>, is said to be a <u>probability function on B</u>-- that is, with &quot;domain&quot; <u>B</u> (that is, the set of propositions of <u>B</u>)and range the closed interval [0,1]. In what follows, reference to an assumed algebra <u>B</u>will be implicit.</p><p>In Appendix 1, I explained how the <u>propositional</u> calculus is applicable to&quot;propositions&quot; understood as sentences or statements as well as to&quot;propositions&quot; understood as factors or properties -- and the same goes for the <u>probability</u>calculus. Roughly speaking, &quot;<u>Pr</u>(<u>X</u>) = <u>r</u>&quot; can be understoodeither as asserting that a <u>sentence or statement</u> <u>X</u> has a probability of <u>r</u>of being <u>true</u> (in a given situation), or as asserting that a <u>factor or property</u>has a probability of <u>r</u> of being <u>exemplified</u> (in a given instance orpopulation). Specifying an interpretation of the propositions is part what must be done to&quot;interpret&quot; a probability function on an algebra; the other part is interpreting&quot;<u>Pr</u>&quot;. Various interpretations of probability (such as frequency, degreeof belief, and partial logical entailment interpretations) are discussed in Chapter 1;here, the focus is on the formal calculus.</p><p>Here are some easy consequences of the probability axioms.</p><p>(4) <u>Pr</u>(~<u>X</u>) = 1 - <u>Pr</u>(<u>X</u>), for all <u>X</u>.</p><p><u>Proof</u>: By (1), <u>Pr</u>(<u>X</u>v~<u>X</u>) = 1; and by (3), <u>Pr</u>(<u>X</u>v~<u>X</u>)= <u>Pr</u>(<u>X</u>) + <u>Pr</u>(~<u>X</u>). So, 1 = <u>Pr</u>(<u>X</u>) + <u>Pr</u>(~<u>X</u>),and thus <u>Pr</u>(~<u>X</u>) = 1 - <u>Pr</u>(<u>X</u>).</p><p>(5) <u>Pr</u>(<u>X</u>) = 0, if <u>X</u> is a contradiction.</p><p><u>Proof</u>: ~<u>X</u> is a tautology, so by (2), <u>Pr</u>(~<u>X</u>) = 1. By (4), <u>Pr</u>(~<u>X</u>)= 1 -Pr(<u>X</u>). So, 1 = 1 - <u>Pr</u>(<u>X</u>), and thus <u>Pr</u>(<u>X</u>) = 0.</p><p>(6) <u>Pr</u>(<u>X</u>) = <u>Pr</u>(<u>Y</u>), if <u>X</u> and <u>Y</u> are logicallyequivalent.</p><p><u>Proof</u>: <u>X</u> and ~<u>Y</u> are mutually exclusive and <u>X</u>v~<u>Y</u> is atautology. So by (2), (3), and (4), 1 = <u>Pr</u>(<u>X</u>v~<u>Y</u>) = <u>Pr</u>(<u>X</u>)+ <u>Pr</u>(~<u>Y</u>) = <u>Pr</u>(<u>X</u>) + 1 - <u>Pr</u>(<u>Y</u>). So, 1 = <u>Pr</u>(<u>X</u>)+ 1 - <u>Pr</u>(<u>Y</u>), and 0 = <u>Pr</u>(<u>X</u>) - <u>Pr</u>(<u>Y</u>), and thus <u>Pr</u>(<u>X</u>)= <u>Pr</u>(<u>Y</u>).</p><p>(7) <u>Pr</u>(<u>X</u>) <u>&lt;</u> <u>Pr</u>(<u>Y</u>), if <u>X</u> logically implies <u>Y</u>.</p><p>(8) 0 <u>&lt;</u> <u>Pr</u>(<u>X</u>) <u>&lt;</u> 1, for all <u>X</u>.</p><p>(9) <u>Pr</u>(<u>X</u>v<u>Y</u>) = <u>Pr</u>(<u>X</u>) + <u>Pr</u>(<u>Y</u>) - <u>Pr</u>(<u>X</u>&amp;<u>Y</u>),for all <u>X</u> and <u>Y</u>.</p><p>The probability of <u>Y</u> <u>conditional on</u> (<u>or given</u>) <u>X</u>, written <u>Pr</u>(<u>Y</u>/<u>X</u>),is defined to be equal to <u>Pr</u>(<u>X</u>&amp;<u>Y</u>)/<u>Pr</u>(<u>X</u>). Note that <u>Pr</u>(<u>Y</u>/<u>X</u>)is defined only when <u>Pr</u>(<u>X</u>) &gt; 0. Since for any <u>X</u> and <u>Y</u>, <u>Pr</u>(<u>X</u>&amp;<u>Y</u>)= <u>Pr</u>(<u>Y</u>&amp;<u>X</u>) (by (6) above), an immediate consequence of thedefinition of conditional probability is what is often called the <u>multiplication rule</u>:</p><p>(9) <u>Pr</u>(<u>X</u>&amp;<u>Y</u>) = <u>Pr</u>(<u>X</u>)<u>Pr</u>(<u>Y</u>/<u>X</u>)= <u>Pr</u>(<u>Y</u>)<u>Pr</u>(<u>X</u>/<u>Y</u>), for all <u>X</u> and <u>Y</u>.</p><p>From (9) follows this simple version of <u>Bayes' theorem</u>:</p><p><u>Pr</u>(<u>Y</u>/<u>X</u>) = <u>Pr</u>(<u>X</u>/<u>Y</u>)<u>Pr</u>(<u>Y</u>)/<u>Pr</u>(<u>X</u>),for all <u>X</u> and <u>Y</u>.</p><p>A proposition <u>Y</u> is said to be <u>probabilistically</u> (or <u>statistically</u>)<u>independent</u> of a proposition <u>X</u> if <u>Pr</u>(<u>Y</u>/<u>X</u>) = <u>Pr</u>(<u>Y</u>).Alternatively, and equivalently, <u>Y</u>'s being probabilistically independent of <u>X</u>can be defined as <u>Pr</u>(<u>X</u>&amp;<u>Y</u>) = <u>Pr</u>(<u>X</u>)<u>Pr</u>(<u>Y</u>).Thus, probabilistic independence is symmetric: if <u>Y</u> is probabilisticallyindependent of <u>X</u>, then <u>X</u> is probabilistically independent of <u>Y</u>, forall <u>X</u> and <u>Y</u>.</p><p>If propositions <u>X</u> and <u>Y</u> are not probabilistically independent, then thereis said to be a <u>probabilistic</u> (or <u>statistical</u>) <u>correlation</u> (or <u>dependence</u>)between <u>X</u> and <u>Y</u>. The correlation is called <u>positive</u> or <u>negative</u>according to whether <u>Pr</u>(<u>Y</u>/<u>X</u>) is greater or less than <u>Pr</u>(<u>Y</u>).This is sometimes described by saying that <u>X</u> is <u>positively or negativelyprobabilistically relevant to Y</u>, or that <u>X has positive or negative probabilisticsignificance for Y</u>. It is easy to see that the following six probabilistic relationsare equivalent:</p><p><u>Pr</u>(<u>Y</u>/<u>X</u>) &gt; <u>Pr</u>(<u>Y</u>);</p><p><u>Pr</u>(<u>X</u>/<u>Y</u>) &gt; <u>Pr</u>(<u>X</u>);</p><p><u>Pr</u>(<u>Y</u>) &gt; <u>Pr</u>(<u>Y</u>/~<u>X</u>);</p><p><u>Pr</u>(<u>X</u>) &gt; <u>Pr</u>(<u>X</u>/~<u>Y</u>);</p><p>Pr(<u>Y</u>/<u>X</u>) &gt; <u>Pr</u>(<u>Y</u>/~<u>X</u>);</p><p><u>Pr</u>(<u>X</u>/<u>Y</u>) &gt; <u>Pr</u>(<u>X</u>/~<u>Y</u>).</p><p>Also, these six relations would remain equivalent if the &quot;&gt;&quot;'s were allreplaced with &quot;&lt;&quot;'s, or with &quot;=&quot;'s. Thus, the two kinds ofprobabilistic correlation (positive and negative), as well as probabilistic independence,are symmetric. If <u>Pr</u>(<u>Y</u>/<u>Z</u>&amp;<u>X</u>) = <u>Pr</u>(<u>Y</u>/<u>Z</u>&amp;~<u>X</u>),then <u>Z</u> is said to <u>screen off</u> any probabilistic correlation of <u>Y</u> with <u>X</u>.</p><p>Two propositions <u>X</u> and <u>Y</u> are called <u>probabilistically equivalent</u>if <u>Pr</u>((<u>X</u>&amp;<u>Y</u>)v(~<u>X</u>&amp;~<u>Y</u>)) = 1. Another way ofputting this is as follows. A common propositional connective, not mentioned in Appendix1, is the <u>biconditional connective</u>, &quot;&lt;-&gt;&quot;. The biconditional of twopropositions <u>X</u> and <u>Y</u> is the proposition that is true just in case <u>X</u>and <u>Y</u> have the same <u>truth value</u> -- that is, either they are both true orthey are both false. The <u>biconditional</u> of <u>X</u> and <u>Y</u> is often expressedas &quot;<u>X</u> if and only if <u>Y</u>&quot;, or, for short, &quot;<u>X</u> iff <u>Y</u>&quot;(<u>X</u> <u>if</u> <u>Y</u>, <u>and</u> <u>X</u> <u>only if</u> <u>Y</u>). Then <u>X</u>and <u>Y</u> are probabilistically equivalent just when <u>Pr</u>(<u>X</u>&lt;-&gt;<u>Y</u>)= 1. When two propositions <u>X</u> and <u>Y</u> are probabilistically equivalent, thenthey are &quot;interchangeable in all probabilistic contexts&quot;. That is, given that <u>X</u>and <u>Y</u> are probabilistically equivalent, if (possibly truth-functionally complex)propositions <u>Z</u>(<u>X</u>,<u>Y</u>) and <u>W</u>(<u>X</u>,<u>Y</u>) result from any(possibly truth-functionally complex) propositions <u>Z</u> and <u>W</u>, respectively, bychanging <u>X</u>'s to <u>Y</u>'s or <u>Y</u>'s to <u>X</u>'s, in any way, then <u>Pr</u>(<u>Z</u>/<u>W</u>)= <u>Pr</u>(<u>Z</u>(<u>X</u>,<u>Y</u>)/<u>W</u>(<u>X</u>,<u>Y</u>)).</p><p>A generalization of the common idea of an average is the statistical idea ofexpectation, or expected value. Given a variable <u>N</u> which can take on the possiblevalues <u>n</u><sub>1</sub>, ..., <u>n</u><sub>_s</sub>, and a probability <u>Pr</u> onpropositions of the form &quot;<u>N</u> = <u>n</u><sub>_i</sub>&quot;, the <u>expectation</u>,or <u>expected value</u>, of <u>N</u> (calculated in terms of the probability <u>Pr</u>)is:</p><p>SUM<sub><u>i</u>=</sub><u><sup>r</sup></u><sub>1</sub> <u>Pr</u>(<u>N</u> = <u>n</u><sub>_i</sub>)<u>n</u><sub>_i</sub>.</p><p>If the probabilities in terms of which an expectation is calculated are conditionalprobabilities, then the expectation is a <u>conditional expectation</u>, or <u>conditionalexpected value</u>. For example, if <u>R</u> is a proposition that may be relevant to thevalue of <u>N</u>, then</p><p>SUM<sub><u>i</u>=</sub><u><sup>r</sup></u><sub>1</sub> <u>Pr</u>(<u>N</u> = <u>n</u><sub>_i</sub>/<u>R</u>)<u>n</u><sub>_i</sub></p><p>is a conditional expectation.</p></body></html>